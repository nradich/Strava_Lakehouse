{"cells":[{"cell_type":"markdown","metadata":{},"source":["Run the config file to authenticate the machine and query variables from Key Vault"]},{"cell_type":"code","execution_count":52,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"17d5c4fa-0e12-483c-a9dc-a0da2cd10381","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Code from file 'file:///c%3A/Users/nicholas.radich/Documents/Strava_Lakehouse/config.py':\n"," client_id = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"clientid\") \r\n","client_secret = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"clientsecret\") \r\n","new_refresh_token = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"newrefreshtoken\")\r\n","activity_id_path = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"activityidpath\") \r\n","historical_activity_id_path = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"historicalactivitydfpath\") \r\n","segment_effort_path = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"segmenteffortpath\") \r\n","segment_details_path = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"segmentdetailspath\") \r\n","\r\n","\r\n","\r\n","import requests\r\n","import urllib3\r\n","urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\r\n","\r\n","auth_url = \"https://www.strava.com/oauth/token\"\r\n","activites_url = \"https://www.strava.com/api/v3/athlete/activities\"\r\n","\r\n","\r\n","payload = {\r\n","    'client_id':  client_id,\r\n","    'client_secret': client_secret,\r\n","    'refresh_token': new_refresh_token,\r\n","    'grant_type': 'refresh_token',\r\n","    'f': 'json'\r\n","}\r\n","\r\n","res = requests.post(auth_url, data=payload, verify=False)\r\n","access_token = res.json()['access_token']"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":[]}],"source":["%run config"]},{"cell_type":"markdown","metadata":{},"source":["Depedencies "]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[]}],"source":["from pyspark.sql.functions import * \n","from pyspark.sql.types import LongType\n","import pandas as pd\n","import requests \n","import utils"]},{"cell_type":"markdown","metadata":{},"source":["API call to grab all of the acitivites within a personal account"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b613fa63-adb9-4938-b65b-4eb3631a5823","showTitle":false,"title":""}},"outputs":[],"source":["#API call to grab all of the acitivites within a personal account\n","def activity_api_call(access_token):\n","    \"\"\"Returns all activities for a personal strava account, need access token\"\"\"\n","    activites_url = \"https://www.strava.com/api/v3/athlete/activities\"\n","    header = {'Authorization': 'Bearer ' + access_token}\n","    param = {'per_page': 200, 'page': 1}\n","    activity_dataset = requests.get(activites_url, headers=header, params=param).json()\n","    \n","    return activity_dataset\n"]},{"cell_type":"markdown","metadata":{},"source":["Make API call to get all strava activites"]},{"cell_type":"code","execution_count":54,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6d9ca4f7-85af-4730-9cc0-d534ef0a9221","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":[]}],"source":["my_dataset = utils.activity_api_call(access_token)"]},{"cell_type":"markdown","metadata":{},"source":["Extracts activity ids and supporting information from initial dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"5ae46e42-7f48-4a1b-8115-8e815367285d","showTitle":false,"title":""}},"outputs":[],"source":["def extract_activities(dataset):\n","    \"\"\"Function to seperate activity_ids and create an activity dataframe. \n","    Returns a df of only the activity ids, and another df with more details about the activiity. \"\"\"\n","\n","    #Empty lists for columns we want to extract\n","    activity_ids = []\n","    start_date = []\n","    activity_name =[]\n","    distance = []\n","    moving_time = []\n","    elapsed_time = []\n","    sport_type = []\n","    total_elevation_gain =[]\n","    count = 0\n","\n","    #a while loop to iterate through the dataset and append values to lists defined above\n","    while count < len(dataset):\n","        activity_ids.append(dataset[count]['id'])\n","        start_date.append(dataset[count]['start_date'])\n","        activity_name.append(dataset[count]['name'])\n","        distance.append(dataset[count]['distance'])\n","        moving_time.append(dataset[count]['moving_time'])\n","        elapsed_time.append(dataset[count]['elapsed_time'])\n","        sport_type.append(dataset[count]['sport_type'])\n","        total_elevation_gain.append(dataset[count]['total_elevation_gain'])\n","        count += 1 \n","        \n","    #convert list to dataframe   \n","    #create a DF of the soley the activity_ids\n","    activity_id_DF = spark.createDataFrame(activity_ids, LongType())\n","    #add file name and timestamp\n","    activity_id_DF = activity_id_DF.withColumnRenamed('value', 'activity_id')\\\n","                                    .withColumn(\"ingest_file_name\", lit(\"activity_ids\")) \\\n","                                    .withColumn(\"ingested_at\", lit(current_timestamp()))\n","    \n","    #columns names for initial DF\n","    #need to specify schema\n","    columns = ['activity_ids','start_date', 'activity_name', 'distance', 'moving_time','elapsed_time', 'sport_type'\\\n","          ,'total_elevation_gain']\n","    #list of lists\n","    #ccombine lists of extract values into list of list\n","    extracted_data = [activity_ids,start_date, activity_name, distance, moving_time,elapsed_time, sport_type\\\n","          ,total_elevation_gain]\n","\n","    #create a pandas Dataframe, then convert to spark to write to storage\n","    #create dataframe from list of list within specified column names\n","    pdf = pd.DataFrame.from_dict(dict(zip(columns, extracted_data)))\n","    activity_df = spark.createDataFrame(pdf)\n","\n","    activity_df = activity_df.withColumn(\"ingest_file_name\", lit(\"activity_information\")) \\\n","                             .withColumn(\"ingested_at\", lit(current_timestamp()))\n","\n","    return activity_id_DF, activity_df"]},{"cell_type":"markdown","metadata":{},"source":["Extract activity IDs and return refined columns "]},{"cell_type":"code","execution_count":55,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2459ffb8-1a9a-4a03-92fa-a7c86cac5fcf","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":[]}],"source":["\n","activity_id_DF, activity_df = utils.extract_activities(my_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["Function to write datasets to storage, delta format"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9d4f172f-5051-4d93-9057-e92313ca31c3","showTitle":false,"title":""}},"outputs":[],"source":["def write_dataframe_to_storage(dataset, storage_path, option, mode ):\n","    \"\"\"Function to write activity ids to storage. Will overwrite current delta file in storage\n","    Option refers to schema overwriteSchema or mergeSchema, mode being either overwrite or append\"\"\"\n","    dataset.write.format(\"delta\")\\\n","    .option(option, \"true\")\\\n","    .mode(mode)\\\n","    .save(storage_path)"]},{"cell_type":"markdown","metadata":{},"source":["Write Activity_ID_DF to storage"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[]}],"source":["#downside dataframes for testing purpose\n","activity_df= activity_df.limit(10)\n","activity_id_DF = activity_id_DF.limit(10)"]},{"cell_type":"code","execution_count":57,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ff90bbb6-be4b-462e-953f-bebff06fcbd8","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":[]}],"source":["#write the activity IDs to storage\n","utils.write_dataframe_to_storage(activity_id_DF,activity_id_path, \"overwriteSchema\",\"overwrite\" )"]},{"cell_type":"markdown","metadata":{},"source":["Read the activities from storage"]},{"cell_type":"code","execution_count":58,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"38b7d57a-e371-40b5-827d-cdd37122276b","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":[]}],"source":["#read the activities from storage\n","stored_activity_ids = spark.read.format(\"delta\").load(activity_id_path)"]},{"cell_type":"markdown","metadata":{},"source":["Compare what is currently in storage vs the most recent API call\n","If nothing is written to storage ie first run, still need to execute and write original DF to storage"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def get_historical_dataset(storagepath, historical_df_to_write, historical_storagepath):\n","    \"\"\"Retrieve record from file path, if nothing exists, insert df to write to storage\"\"\"\n","    from pyspark.sql.utils import AnalysisException\n","    try:\n","        #try to read from storage\n","        historical_dataframe = spark.read.format(\"delta\").load(historical_storagepath)\n","    except: \n","        try:\n","        #if that fails, write the submitted dataframe to storage\n","            write_dataframe_to_storage(historical_df_to_write,storagepath, \"mergeSchema\", \"append\" )\n","        finally:\n","            historical_dataframe = spark.read.format(\"delta\").load(storagepath)\n","\n","    return historical_dataframe"]},{"cell_type":"markdown","metadata":{},"source":["Write full dataset to storage, will check to see if data already exists at path "]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[]}],"source":["historical_activites = utils.get_historical_dataset(historical_activity_id_path, activity_df, historical_activity_id_path)"]},{"cell_type":"markdown","metadata":{},"source":["Extract those activites not currently in storage"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[]}],"source":["#Extract activity ids from dataframes to list, to make comparison \n","#activity_id_list = activity_id_DF.select('activity_ids').distinct().collect()\n","#from full dataset, all unique activity ids\n","activity_id_list = stored_activity_ids.select('activity_id').distinct().rdd.flatMap(lambda x: x).collect()\n","\n","#currently what is written in storage, activity IDs\n","historical_activity_id_list = historical_activites.select('activity_ids').distinct().rdd.flatMap(lambda x: x).collect()\n","\n","#make comparison between two, and find those IDs not written to storage\n","activity_ids_not_written_to_storage = [x for x in activity_id_list if x not in historical_activity_id_list ]\n","\n","#filter original DF with those ids not currently in storage\n","new_activities = activity_df.filter(activity_df.activity_ids.isin(activity_ids_not_written_to_storage))"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"data":{"text/plain":["+------------+--------------------+------------------+--------+-----------+------------+----------+--------------------+--------------------+--------------------+\n","|activity_ids|          start_date|     activity_name|distance|moving_time|elapsed_time|sport_type|total_elevation_gain|    ingest_file_name|         ingested_at|\n","+------------+--------------------+------------------+--------+-----------+------------+----------+--------------------+--------------------+--------------------+\n","|  9709076171|2023-08-24T00:34:56Z|              laps|  2591.0|        901|         933|       Run|                 7.7|activity_information|2023-08-24 17:10:...|\n","|  9663381569|2023-08-16T23:33:23Z|Torrey Pines North|  4876.1|       3264|       10967|      Golf|                33.8|activity_information|2023-08-24 17:10:...|\n","|  9656452945|2023-08-16T00:06:24Z|      evening ride| 20579.3|       3888|        5220|      Ride|               158.6|activity_information|2023-08-24 17:10:...|\n","+------------+--------------------+------------------+--------+-----------+------------+----------+--------------------+--------------------+--------------------+"]},"metadata":{},"output_type":"display_data"}],"source":["new_activities.show()"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[]}],"source":["#append new activity IDs to the storage location\n","utils.write_dataframe_to_storage(new_activities, historical_activity_id_path,\"mergeSchema\", \"append\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df1 = spark.read.format(\"delta\").load(historical_activity_id_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#only adds in distinct activities\n","from pyspark.sql import functions as F\n","df1.orderBy(F.desc('activity_ids')).show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["activity_id_DF.select('activity_id').show()"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"Query_Activities","notebookOrigID":4431027783720630,"widgets":{}},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"vscode":{"interpreter":{"hash":"89bbb57337a288069efe3ede2e44e349d48d03d33172adbe5738fcfdbda01bd0"}}},"nbformat":4,"nbformat_minor":0}
