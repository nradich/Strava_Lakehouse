{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Code from file 'file:///c%3A/Users/nicholas.radich/Documents/Strava_Lakehouse/config.py':\n",
       " client_id = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"clientid\") \r\n",
       "client_secret = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"clientsecret\") \r\n",
       "new_refresh_token = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"newrefreshtoken\")\r\n",
       "activity_id_path = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"activityidpath\") \r\n",
       "historical_activity_id_path = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"historicalactivitydfpath\") \r\n",
       "segment_effort_path = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"segmenteffortpath\") \r\n",
       "segment_details_path = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"segmentdetailspath\") \r\n",
       "\r\n",
       "\r\n",
       "\r\n",
       "import requests\r\n",
       "import urllib3\r\n",
       "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\r\n",
       "\r\n",
       "auth_url = \"https://www.strava.com/oauth/token\"\r\n",
       "activites_url = \"https://www.strava.com/api/v3/athlete/activities\"\r\n",
       "\r\n",
       "\r\n",
       "payload = {\r\n",
       "    'client_id':  client_id,\r\n",
       "    'client_secret': client_secret,\r\n",
       "    'refresh_token': new_refresh_token,\r\n",
       "    'grant_type': 'refresh_token',\r\n",
       "    'f': 'json'\r\n",
       "}\r\n",
       "\r\n",
       "res = requests.post(auth_url, data=payload, verify=False)\r\n",
       "access_token = res.json()['access_token']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "%run config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from pyspark.sql.functions import * \n",
    "import pandas as pd\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic to orchestrate querying of Strava API\n",
    "\n",
    "1. Raw query of API to attain all activites currently showing\n",
    "2. Query DBFS for all activities written to storage\n",
    "3. Make comparison, and return those activities not wrriten to storage, and write to storage\n",
    "    - 3a. If all written to storage continue to next comparison\n",
    "4.  Will now need to compare activities and their associated segments ie each acitvity may have double digit segments\n",
    "    - 4a. Compare what is in storage vs API call, query the details for those not in storage, and write to storage\n",
    "\n",
    "5. Finally, compare segment details (name, distance, pr) in storage vs current API call.\n",
    "    - 5a  Query details for those not in storage, will need to stay under 99 API call limit\n",
    "    - 5b. Longest step due to call limits\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Api Call for Personal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "api_starting_limit = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#authenticate to API\n",
    "my_dataset = utils.activity_api_call(access_token)\n",
    "#grab activity ids, and the more information about the activities\n",
    "activity_id_DF, activity_df = utils.extract_activities(my_dataset)\n",
    "\n",
    "#stored activitity ids is where we will compare what we queries in the api, vs what is in storage\n",
    "stored_activity_ids = spark.read.format(\"delta\").load(activity_id_path)\n",
    "\n",
    "#islote the distinct activity Ids from each dataframe\n",
    "activity_id_list_in_storage = stored_activity_ids.select('activity_id').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "activity_ids_from_API = activity_id_DF.select('activity_id').distinct().rdd.flatMap(lambda x:x).collect()\n",
    "\n",
    "#find activities not writtent to storage\n",
    "activity_ids_not_in_storage = utils.list_comparison(activity_id_list_in_storage,activity_ids_from_API )\n",
    "\n",
    "\n",
    "#ensure that there are activites to write to storage, otherwise continue\n",
    "if len(activity_ids_not_in_storage) >0:\n",
    "\n",
    "    #take ids not writtent to storage from activity_df and filter them\n",
    "    new_activities = activity_df.filter(activity_df.activity_ids.isin(activity_ids_not_in_storage))\n",
    "    new_ids = activity_id_DF.filter(activity_id_DF.activity_id.isin(activity_ids_not_in_storage))\n",
    "\n",
    "    #write new activities to storage, ensure no duplicates\n",
    "    utils.write_dataframe_to_storage(new_activities,historical_activity_id_path, \"mergeSchema\", \"append\" )\n",
    "    utils.write_dataframe_to_storage(new_ids,activity_id_path, \"mergeSchema\", \"append\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Activities and Segment IDS for each activitiy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#activity ids in storage post compare\n",
    "stored_activity_ids = spark.read.format(\"delta\").load(activity_id_path)\n",
    "\n",
    "#activity IDs with segments in storage, \n",
    "segments_in_storage = spark.read.format(\"delta\").load(segment_effort_path)\n",
    "\n",
    "#isolte activity_ids\n",
    "activities_with_segments_in_storage = segments_in_storage.select('activity_id').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "activity_ids_all = stored_activity_ids.select('activity_id').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "#compare the two\n",
    "activities_without_segments = utils.list_comparison(activities_with_segments_in_storage, activity_ids_all)\n",
    "#returns activities with out semgment information\n",
    "#now need to query segment info, will probably hit 99 request limit\n",
    "\n",
    "\n",
    "if len(activities_without_segments) > 0: \n",
    "    #condition that if there are activities to query, do it\n",
    "    # Need to limit calls to 99 to not exceed API\n",
    "    activities_without_segments =   activities_without_segments[:api_starting_limit]    \n",
    "\n",
    "    #got activities and semgents\n",
    "    segment_id_df = utils.query_segments(activities_without_segments, access_token)\n",
    "\n",
    "    #need to add in activities that don' have segments\n",
    "    returned_activity_ids = segment_id_df.select(\"activity_id\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    activities_no_segments = utils.list_comparison(returned_activity_ids, activities_without_segments)\n",
    "    #there are no activites with no segments to query\n",
    "    if len(activities_no_segments) > 0:\n",
    "        all_activities_with_segments = utils.append_activities_without_segments(segment_id_df,activities_no_segments )\n",
    "    else:\n",
    "        all_activities_with_segments = segment_id_df\n",
    "\n",
    "    #write all queried segments to storage\n",
    "    utils.write_dataframe_to_storage(all_activities_with_segments, segment_effort_path, \"mergeSchema\", \"append\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deducted used calls from activity queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "post_activity_api_limit = api_starting_limit - len(activities_without_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Segment details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#segments with activities \n",
    "all_segments = spark.read.load(segment_effort_path)\n",
    "#isolate segment IDS\n",
    "all_segment_ids = all_segments.select(\"segment_id\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# segment details\n",
    "segment_details = spark.read.load(segment_details_path)\n",
    "segment_details_ids = segment_details.select(\"returned_segment\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "#extract segment id values\n",
    "\n",
    "#comparison to feed to function\n",
    "segment_details_to_query = utils.list_comparison(segment_details_ids, all_segment_ids)\n",
    "\n",
    "#grab the max amount of segments we can query within the API contraints\n",
    "if len(segment_details_to_query) > 0  and post_activity_api_limit > 0 :\n",
    "    api_limit_segments = segment_details_to_query[:post_activity_api_limit]\n",
    "    #had to removed 'none' values from the list \n",
    "    api_limit_segments_filtered =[x for x in api_limit_segments if x is not None]\n",
    "    #only query segment details if necessary \n",
    "    if len(api_limit_segments_filtered) > 0 : \n",
    "        returned_segment_details = utils.query_segment_details(api_limit_segments_filtered, access_token)\n",
    "        #write to storage \n",
    "        utils.write_dataframe_to_storage(returned_segment_details,segment_details_path, \"mergeSchema\", \"append\" )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
