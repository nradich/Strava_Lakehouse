{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Code from file 'file:///c%3A/Users/nicholas.radich/Documents/Strava_Lakehouse/config.py':\n",
       " client_id = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"clientid\") \r\n",
       "client_secret = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"clientsecret\") \r\n",
       "new_refresh_token = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"newrefreshtoken\")\r\n",
       "activity_id_path = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"activityidpath\") \r\n",
       "historical_activity_id_path = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"historicalactivitydfpath\") \r\n",
       "segment_effort_path = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"segmenteffortpath\") \r\n",
       "segment_details_path = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"segmentdetailspath\") \r\n",
       "\r\n",
       "\r\n",
       "\r\n",
       "import requests\r\n",
       "import urllib3\r\n",
       "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\r\n",
       "\r\n",
       "auth_url = \"https://www.strava.com/oauth/token\"\r\n",
       "activites_url = \"https://www.strava.com/api/v3/athlete/activities\"\r\n",
       "\r\n",
       "\r\n",
       "payload = {\r\n",
       "    'client_id':  client_id,\r\n",
       "    'client_secret': client_secret,\r\n",
       "    'refresh_token': new_refresh_token,\r\n",
       "    'grant_type': 'refresh_token',\r\n",
       "    'f': 'json'\r\n",
       "}\r\n",
       "\r\n",
       "res = requests.post(auth_url, data=payload, verify=False)\r\n",
       "access_token = res.json()['access_token']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "%run config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from pyspark.sql.functions import * \n",
    "import pandas as pd\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic to orchestrate querying of Strava API\n",
    "\n",
    "1. Raw query of API to attain all activites currently showing\n",
    "2. Query DBFS for all activities written to storage\n",
    "3. Make comparison, and return those activities not wrriten to storage\n",
    "    - 3a. If all written to storage continue to next comparison\n",
    "4. For those not written storage, query API and write to storage\n",
    "    - Repeat 3 to ensure all activites are written\n",
    "\n",
    "5.  Will now need to compare activities and their associated segments ie each acitvity may have double digit segments\n",
    "\n",
    "6. Looking at query segment notebook for that comparison - checking all activity IDs have been queried\n",
    "    - 6a  IF no continue queries, if yes continue to segment details\n",
    "\n",
    "7. Segment details, will take the longest. Ensure that all segments and their associated details have been queried.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "activity_id_list = [9663381569,9656452945,9635250821, 9578982519,9559341308,9515669005,9408871895\n",
    ",9298361043,9248492217,9235757648]\n",
    "\n",
    "activity_id_subset = [9663381569,9656452945,9635250821]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "activity_ids_not_written_to_storage = [x for x in activity_id_list if x not in activity_id_subset ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "All activities in storage"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if len(activity_ids_not_written_to_storage) == 0:\n",
    "    print (\"All activities in storage\")\n",
    "else:\n",
    "    print( f\"Need to query {len(activity_ids_not_written_to_storage)}, activities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make api call to strava API\n",
    "#from query_activities notebook\n",
    "my_dataset = utils.activity_api_call(access_token)\n",
    "\n",
    "#extract the activities\n",
    "activity_id_DF, activity_df = extract_activities(my_dataset)\n",
    "\n",
    "#read in historical activities \n",
    "stored_activity_ids = spark.read.format(\"delta\").load(activity_id_path)\n",
    "activity_id_list = stored_activity_ids.select('activity_id').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "\n",
    "#make comparison between stored and queries\n",
    "#need to convert activity_id_DF to a list of IDS\n",
    "#activity_ids_not_written_to_storage = [x for x in activity_id_list if x not in historical_activity_id_list ]\n",
    "\n",
    "\n",
    "activity_ids_not_written_to_storage = [x for x in activity_id_subset if x not in activity_id_list ]\n",
    "\n",
    "\n",
    "\n",
    "#if we do not have all activities written to storage\n",
    "if len(activity_ids_not_written_to_storage) = 0:\n",
    "    continue\n",
    "#take those and query\n",
    "else: new_activities extract_activities(activity_ids_not_written_to_storage) \n",
    "& write_to_storage(new_activities)\n",
    "\n",
    "done:\n",
    "\n",
    "Run check again ie if len(activity_ids_not_written_to_storage) = 0:\n",
    "#Then compare activities with segments to those written in storage\n",
    "\n",
    "#now looking at 'Query_Segment' notebook \n",
    "#will need to compare the activity ids that have already been queried for their segments\n",
    "segments_in_storage = spark.read.format(\"delta\").load(segment_effort_path)\n",
    "activity_ids_with_queried_segments = segments_in_storage.select('activity_id').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "\n",
    "activity_ids_without_queried_segments = [x for x in full_activity_ids  if x not in activity_ids_with_queried_segments ]\n",
    "\n",
    "#will also need to incorporate activities without segments into the results\n",
    "\n",
    "#querying historical segments\n",
    "#will need to make sure that \n",
    "segments_in_storage = spark.read.format(\"delta\").load(segment_effort_path)\n",
    "\n",
    "#make that comparison\n",
    "\n",
    "#will now need to get the segment details, from 'segment_exploration' notebook\n",
    "segment_df = query_segment_details(segment_list)\n",
    "\n",
    "#again making comparison\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
