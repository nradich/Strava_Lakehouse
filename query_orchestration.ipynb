{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * \n",
    "import pandas as pd\n",
    "import utils\n",
    "from ratelimiter import RateLimiter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic to orchestrate querying of Strava API\n",
    "\n",
    "1. Raw query of API to attain all activites currently showing\n",
    "2. Query DBFS for all activities written to storage\n",
    "3. Make comparison, and return those activities not wrriten to storage\n",
    "    - 3a. If all written to storage continue to next comparison\n",
    "4. For those not written storage, query API and write to storage\n",
    "    - Repeat 3 to ensure all activites are written\n",
    "\n",
    "5.  Will now need to compare activities and their associated segments ie each acitvity may have double digit segments\n",
    "\n",
    "6. Looking at query segment notebook for that comparison - checking all activity IDs have been queried\n",
    "    - 6a  IF no continue queries, if yes continue to segment details\n",
    "\n",
    "7. Segment details, will take the longest. Ensure that all segments and their associated details have been queried.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Makes 1 API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#authenticate to API\n",
    "my_dataset = utils.activity_api_call(access_token)\n",
    "#grab activity ids, and the more information about the activities\n",
    "activity_id_DF, activity_df = utils.extract_activities(my_dataset)\n",
    "\n",
    "#stored activitity ids is where we will compare what we queries in the api, vs what is in storage\n",
    "stored_activity_ids = spark.read.format(\"delta\").load(activity_id_path)\n",
    "\n",
    "#islote the distinct activity Ids from each dataframe\n",
    "activity_id_list_in_storage = stored_activity_ids.select('activity_id').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "activity_ids_from_API = activity_id_DF.select('activity_id').distinct().rdd.flatMap(lambda x:x).collect()\n",
    "\n",
    "#find activities not writtent to storage\n",
    "activity_ids_not_in_storage = utils.list_comparison(activity_id_list_in_storage,activity_ids_from_API )\n",
    "\n",
    "#take ids not writtent to storage from activity_df and filter them\n",
    "new_activities = activity_df.filter(activity_df.activity_ids.isin(activity_ids_not_in_storage))\n",
    "new_ids = activity_id_DF.filter(activity_id_DF.activity_id.isin(activity_ids_not_in_storage))\n",
    "\n",
    "#write new activities to storage, ensure no duplicates\n",
    "utils.write_dataframe_to_storage(new_activities,historical_activity_id_path, \"mergeSchema\", \"append\" )\n",
    "utils.write_dataframe_to_storage(new_ids,activity_id_path, \"mergeSchema\", \"append\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable number of api call, could do the math ie subtract from rate counter at the top\n",
    "#### 30 in this particular call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(activities_without_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activity ids in storage post compare\n",
    "stored_activity_ids = spark.read.format(\"delta\").load(activity_id_path)\n",
    "\n",
    "#activity IDs with segments in storage, \n",
    "segments_in_storage = spark.read.format(\"delta\").load(segment_effort_path)\n",
    "\n",
    "#isolte activity_ids\n",
    "activities_with_segments_in_storage = segments_in_storage.select('activity_id').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "activity_ids_all = stored_activity_ids.select('activity_id').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "#compare the two\n",
    "activities_without_segments = utils.list_comparison(activities_with_segments_in_storage, activity_ids_all)\n",
    "#returns activities with out semgment information\n",
    "#now need to query segment info, will probably hit 99 request limit\n",
    "\n",
    "if len(activities_without_segments) > 0: \n",
    "    #condition that if there are activities to query, do it, otherwise continue on       \n",
    "\n",
    "    #got activities and semgents\n",
    "    segment_id_df = utils.query_segments(activities_without_segments, access_token)\n",
    "\n",
    "    #need to add in activities that don' have segments\n",
    "    returned_activity_ids = segment_id_df.select(\"activity_id\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    activities_no_segments = utils.list_comparison(returned_activity_ids, activities_without_segments)\n",
    "    all_activities_with_segments = utils.append_activities_without_segments(segment_id_df,activities_no_segments )\n",
    "\n",
    "    #write all queried segments to storage\n",
    "    utils.write_dataframe_to_storage(all_activities_with_segments, segment_effort_path, \"mergeSchema\", \"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section for querying segment information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segments with activities \n",
    "all_segments = spark.read.load(segment_effort_path)\n",
    "#isolate segment IDS\n",
    "all_segment_ids = all_segments.select(\"segment_id\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# segment details\n",
    "segment_details = spark.read.load(segment_details_path)\n",
    "segment_details_ids = segment_details.select(\"returned_segment\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "#extract segment id values\n",
    "\n",
    "#comparison to feed to function\n",
    "segment_details_to_query = utils.list_comparison(segment_details_ids, all_segment_ids)\n",
    "\n",
    "rate_limiter = RateLimiter(max_calls=20)\n",
    "test_df = utils.query_segment_details_with_limits(segment_details_to_query, access_token, rate_limiter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scratch Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = segment_details_to_query[30:45]\n",
    "rate_limiter = RateLimiter(max_calls=25)\n",
    "test_df_SHOULD_BE5 = query_segment_details_with_limits(subset, access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_SHOULD_BE5.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in subset:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#make comparison\n",
    "segment_details_to_query = utils.list_comparison(segment_details_ids, all_segment_ids)\n",
    "subset = segment_details_to_query[:25]\n",
    "\n",
    "recent_segment_details = utils.query_segment_details( subset, access_token)\n",
    "\n",
    "#write these new segments to storage\n",
    "utils.write_dataframe_to_storage(recent_segment_details, segment_details_path, \"mergeSchema\", \"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in segment_details_to_query:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_in_storage = spark.read.format(\"delta\").load(segment_effort_path)\n",
    "segments_in_storage.orderby('activity_id').desc()display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_ids_with_queried_segments = segments_in_storage.select('activity_id').distinct().rdd.flatMap(lambda x: x).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will need to compare the activity ids that have already been queried for their segments\n",
    "segments_in_storage = spark.read.format(\"delta\").load(segment_effort_path)\n",
    "activity_ids_with_queried_segments = segments_in_storage.select('activity_id').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "\n",
    "activity_ids_without_queried_segments = [x for x in full_activity_ids  if x not in activity_ids_with_queried_segments ]\n",
    "\n",
    "#grab the first 99 spots so as not to overload the api call\n",
    "eligible_activities = activity_ids_without_queried_segments[:99]\n",
    "\n",
    "\n",
    "\n",
    "segment_id_df = utils.query_segments(eligible_activities, access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_id_list = [9663381569,9656452945,9635250821, 9578982519,9559341308,9515669005,9408871895\n",
    ",9298361043,9248492217,9235757648]\n",
    "\n",
    "activity_id_subset = [9663381569,9656452945,9635250821]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_ids_not_written_to_storage = [x for x in activity_id_list if x not in activity_id_subset ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(activity_ids_not_written_to_storage) == 0:\n",
    "    print (\"All activities in storage\")\n",
    "else:\n",
    "    print( f\"Need to query {len(activity_ids_not_written_to_storage)}, activities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make api call to strava API\n",
    "#from query_activities notebook\n",
    "my_dataset = utils.activity_api_call(access_token)\n",
    "\n",
    "#extract the activities\n",
    "activity_id_DF, activity_df = extract_activities(my_dataset)\n",
    "\n",
    "#read in historical activities \n",
    "stored_activity_ids = spark.read.format(\"delta\").load(activity_id_path)\n",
    "activity_id_list = stored_activity_ids.select('activity_id').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "\n",
    "#make comparison between stored and queries\n",
    "#need to convert activity_id_DF to a list of IDS\n",
    "#activity_ids_not_written_to_storage = [x for x in activity_id_list if x not in historical_activity_id_list ]\n",
    "\n",
    "\n",
    "activity_ids_not_written_to_storage = [x for x in activity_id_subset if x not in activity_id_list ]\n",
    "\n",
    "\n",
    "\n",
    "#if we do not have all activities written to storage\n",
    "if len(activity_ids_not_written_to_storage) = 0:\n",
    "    continue\n",
    "#take those and query\n",
    "else: new_activities extract_activities(activity_ids_not_written_to_storage) \n",
    "& write_to_storage(new_activities)\n",
    "\n",
    "done:\n",
    "\n",
    "Run check again ie if len(activity_ids_not_written_to_storage) = 0:\n",
    "#Then compare activities with segments to those written in storage\n",
    "\n",
    "#now looking at 'Query_Segment' notebook \n",
    "#will need to compare the activity ids that have already been queried for their segments\n",
    "segments_in_storage = spark.read.format(\"delta\").load(segment_effort_path)\n",
    "activity_ids_with_queried_segments = segments_in_storage.select('activity_id').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "\n",
    "activity_ids_without_queried_segments = [x for x in full_activity_ids  if x not in activity_ids_with_queried_segments ]\n",
    "\n",
    "#will also need to incorporate activities without segments into the results\n",
    "\n",
    "#querying historical segments\n",
    "#will need to make sure that \n",
    "segments_in_storage = spark.read.format(\"delta\").load(segment_effort_path)\n",
    "\n",
    "#make that comparison\n",
    "\n",
    "#will now need to get the segment details, from 'segment_exploration' notebook\n",
    "segment_df = query_segment_details(segment_list)\n",
    "\n",
    "#again making comparison\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
