{"cells":[{"cell_type":"markdown","metadata":{},"source":["Run the config file to authenticate script and query Key Vault\n","\n","Second script. Run after 'Query_Activites'"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[{"data":{"text/plain":["Code from file 'file:///c%3A/Users/nicholas.radich/Documents/Strava_Lakehouse/config.py':\n"," client_id = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"clientid\") \r\n","client_secret = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"clientsecret\") \r\n","new_refresh_token = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"newrefreshtoken\")\r\n","activity_id_path = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"activityidpath\") \r\n","historical_activity_id_path = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"historicalactivitydfpath\") \r\n","segment_effort_path = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"segmenteffortpath\") \r\n","segment_details_path = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"segmentdetailspath\") \r\n","\r\n","\r\n","\r\n","import requests\r\n","import urllib3\r\n","urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\r\n","\r\n","auth_url = \"https://www.strava.com/oauth/token\"\r\n","activites_url = \"https://www.strava.com/api/v3/athlete/activities\"\r\n","\r\n","\r\n","payload = {\r\n","    'client_id':  client_id,\r\n","    'client_secret': client_secret,\r\n","    'refresh_token': new_refresh_token,\r\n","    'grant_type': 'refresh_token',\r\n","    'f': 'json'\r\n","}\r\n","\r\n","res = requests.post(auth_url, data=payload, verify=False)\r\n","access_token = res.json()['access_token']"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":[]}],"source":["%run config"]},{"cell_type":"markdown","metadata":{},"source":["Dependencies "]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[]}],"source":["from pyspark.sql.functions import * \n","from pyspark.sql import functions as F\n","from pyspark.sql import Row\n","import pandas as pd\n","import utils"]},{"cell_type":"markdown","metadata":{},"source":["### API Query to get more specific details for each activity, need to pass each activity off individually "]},{"cell_type":"markdown","metadata":{},"source":["Get full activity dataset from what is written in storage, should be all activites"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[]}],"source":["\n","full_activity_dataset = spark.read.format(\"delta\").load(historical_activity_id_path)"]},{"cell_type":"markdown","metadata":{},"source":["Grab all of the disinct activity IDs "]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[{"data":{"text/plain":["147"]},"metadata":{},"output_type":"display_data"}],"source":["full_activity_dataset.count()"]},{"cell_type":"code","execution_count":79,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"920f4af1-ce5f-48a7-8372-4725ed606ca0","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":[]}],"source":["full_activity_ids = full_activity_dataset.select('activity_ids').distinct().rdd.flatMap(lambda x: x).collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def query_segments(activity_ids : list):\n","    \"\"\"Gets all segment_ids for each activity_id submitted\n","    Returns distinct values\"\"\"\n","    df = pd.DataFrame()\n","    activity_id_list =[]\n","    segment_id_list =[]\n","    for id in activity_ids:\n","        activity_id_urls = (\"{}{}?include_all_efforts= True\").format(\"https://www.strava.com/api/v3/activities/\",id)\n","        header = {'Authorization': 'Bearer ' + access_token}\n","        param = {'per_page': 200, 'page': 1}\n","        my_activity = requests.get(activity_id_urls, headers=header, params=param).json()\n","\n","        segment_effort_count =  len(my_activity['segment_efforts'])\n","        count = 0\n","        while count < segment_effort_count:\n","\n","            activity_id = my_activity['segment_efforts'][count]['activity']['id']\n","            segment_id = my_activity['segment_efforts'][count]['id']\n","            activity_id_list.append(activity_id)\n","            segment_id_list.append(segment_id)\n","                  \n","            columns = ['segment_id', 'activity_id']\n","            extracted_data = [segment_id_list, activity_id_list]\n","            segment_df = pd.DataFrame.from_dict(dict(zip(columns, extracted_data)))\n","\n","            df = pd.concat([df, segment_df])\n","\n","            count += 1\n","\n","    #convert pandas df to spark\n","        \n","    segment_spark_df = spark.createDataFrame(df)\n","\n","    #drop duplicate entries\n","    segment_spark_df = segment_spark_df.dropDuplicates()\n","\n","    segment_spark_df = segment_spark_df.select(concat(segment_spark_df.segment_id,segment_spark_df.activity_id).alias(\"Activity_Segment_JointID\"), 'segment_id','activity_id')\n","\n","    segment_spark_df = segment_spark_df.withColumn(\"ingest_file_name\", lit(\"segment_efforts_ids\")) \\\n","                                .withColumn(\"ingested_at\", lit(current_timestamp()))\n","\n","    return segment_spark_df"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[{"data":{"text/html":["<span class='ansi-red-fg'>NameError</span>: name 'access_token' is not defined"]},"metadata":{},"output_type":"display_data"},{"ename":"Error","evalue":"\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\nFile \u001b[0;32m<command--1>:13\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#grab the first 99 spots so as not to overload the api call\u001b[39;00m\n\u001b[1;32m      9\u001b[0m eligible_activities \u001b[38;5;241m=\u001b[39m activity_ids_without_queried_segments[:\u001b[38;5;241m99\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m segment_id_df \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mquery_segments(eligible_activities)\n\nFile \u001b[0;32m/Workspace/Users/nicholas.radich@avanade.com/.ide/Strava_Lakehouse-ef0b9980/utils.py:109\u001b[0m, in \u001b[0;36mquery_segments\u001b[0;34m(activity_ids)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m activity_ids:\n\u001b[1;32m    108\u001b[0m     activity_id_urls \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m?include_all_efforts= True\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.strava.com/api/v3/activities/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mid\u001b[39m)\n\u001b[0;32m--> 109\u001b[0m     header \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43maccess_token\u001b[49m}\n\u001b[1;32m    110\u001b[0m     param \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mper_page\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m200\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m}\n\u001b[1;32m    111\u001b[0m     my_activity \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(activity_id_urls, headers\u001b[38;5;241m=\u001b[39mheader, params\u001b[38;5;241m=\u001b[39mparam)\u001b[38;5;241m.\u001b[39mjson()\n\n\u001b[0;31mNameError\u001b[0m: name 'access_token' is not defined","output_type":"error","traceback":["Error: \u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n","File \u001b[0;32m<command--1>:13\u001b[0m\n","\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#grab the first 99 spots so as not to overload the api call\u001b[39;00m\n","\u001b[1;32m      9\u001b[0m eligible_activities \u001b[38;5;241m=\u001b[39m activity_ids_without_queried_segments[:\u001b[38;5;241m99\u001b[39m]\n","\u001b[0;32m---> 13\u001b[0m segment_id_df \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mquery_segments(eligible_activities)\n","\n","File \u001b[0;32m/Workspace/Users/nicholas.radich@avanade.com/.ide/Strava_Lakehouse-ef0b9980/utils.py:109\u001b[0m, in \u001b[0;36mquery_segments\u001b[0;34m(activity_ids)\u001b[0m\n","\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m activity_ids:\n","\u001b[1;32m    108\u001b[0m     activity_id_urls \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m?include_all_efforts= True\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.strava.com/api/v3/activities/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mid\u001b[39m)\n","\u001b[0;32m--> 109\u001b[0m     header \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43maccess_token\u001b[49m}\n","\u001b[1;32m    110\u001b[0m     param \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mper_page\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m200\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m}\n","\u001b[1;32m    111\u001b[0m     my_activity \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(activity_id_urls, headers\u001b[38;5;241m=\u001b[39mheader, params\u001b[38;5;241m=\u001b[39mparam)\u001b[38;5;241m.\u001b[39mjson()\n","\n","\u001b[0;31mNameError\u001b[0m: name 'access_token' is not defined\n","\tat d._doExecution (c:\\Users\\nicholas.radich\\.vscode\\extensions\\paiqo.databricks-vscode-2.2.2\\dist\\node\\extension.js:2:66445)\n","\tat process.processTicksAndRejections (node:internal/process/task_queues:96:5)\n","\tat async d.executeHandler (c:\\Users\\nicholas.radich\\.vscode\\extensions\\paiqo.databricks-vscode-2.2.2\\dist\\node\\extension.js:2:55403)\n","\tat async y.$executeCells (c:\\Users\\nicholas.radich\\AppData\\Local\\Programs\\Microsoft VS Code\\resources\\app\\out\\vs\\workbench\\api\\node\\extensionHostProcess.js:104:100503)"]}],"source":["#will need to compare the activity ids that have already been queried for their segments\n","segments_in_storage = spark.read.format(\"delta\").load(segment_effort_path)\n","activity_ids_with_queried_segments = segments_in_storage.select('activity_id').distinct().rdd.flatMap(lambda x: x).collect()\n","\n","\n","activity_ids_without_queried_segments = [x for x in full_activity_ids  if x not in activity_ids_with_queried_segments ]\n","\n","#grab the first 99 spots so as not to overload the api call\n","eligible_activities = activity_ids_without_queried_segments[:99]\n","\n","\n","\n","segment_id_df = utils.query_segments(eligible_activities, config.access_token)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#activities that returned segments\n","#need to add in the lambda to get the row values\n","returned_activity_ids = segment_id_df.select(\"activity_id\").distinct().rdd.flatMap(lambda x: x).collect()\n","\n","#activies submitted, that did not return segments\n","activity_ids_without_segments = [x for x in eligible_activities if x not in returned_activity_ids ]\n","\n","#Append in activies without segments to the DF\n","#convert the list of ids_without segments into a DF\n","#need to adjust column names \n","rows = [Row(Activity_Segment_JointID=i,  activity_id = i) for i in activity_ids_without_segments]\n","new_df = spark.createDataFrame(rows)\n","new_df = new_df.withColumn(\"ingest_file_name\", lit(\"segment_efforts_ids\")) \\\n","                                .withColumn(\"ingested_at\", lit(current_timestamp()))\\\n","                                .withColumn(\"segment_id\", lit(None).cast(\"long\"))\n","\n","#reorder columns to union into \n","new_df_reordered = new_df.select(*segment_id_df.columns)\n","\n","#union the two datasets together\n","all_segment_ids = segment_id_df.union(new_df_reordered)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["segment_id_df.show(20)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["value_counts = segment_id_df.groupBy(\"ingested_at\").agg(F.count(\"ingested_at\").alias(\"count\")).orderBy(\"count\", ascending=False)\n","value_counts.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_segment_ids.select(\"activity_id\").distinct().count()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utils.write_dataframe_to_storage(all_segment_ids,segment_effort_path, \"mergeSchema\", \"append\" )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_segment_ids.show(20)"]},{"cell_type":"markdown","metadata":{},"source":["Take unique activity ids, and extract all of the segments associated with those activities"]},{"cell_type":"markdown","metadata":{},"source":["Need to compare activites already stored with segments as there are limits for strava API\n","\n","Initially will go 99 request, but might need to reduce that to save requests for segments"]},{"cell_type":"markdown","metadata":{},"source":["Query all segments from all activities "]},{"cell_type":"markdown","metadata":{},"source":["Not all activities register segments, will need to append into final DF that is writtent to storage with placerholder values so as not to keep querying them"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["segment_id_df.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_segment_ids.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["segments_in_storage = spark.read.format(\"delta\").load(segment_effort_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utils.write_dataframe_to_storage(all_segment_ids,segment_effort_path, \"mergeSchema\", \"append\" )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["segments_in_storage.show(10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["try:\n","    #Query path, see if there are any activities with their associated segments written to storage\n","    segments_in_storage = spark.read.format(\"delta\").load(segment_effort_path)\n","except:\n","    #if that errors, meaning first time running the script\n","    #Write the first 99 activites to storage, will need to specificy sort order \n","    top_99_activity_ids = \n","    utils.write_dataframe_to_storage(historical_df_to_write,storagepath, \"mergeSchema\", \"append\" )\n"]},{"cell_type":"markdown","metadata":{},"source":["Need to limit to 100 request as the api throws an errors after"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utils.write_dataframe_to_storage(segment_id_df,segment_effort_path, \"overwriteSchema\",\"overwrite\" )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["segments_in_storage = spark.read.format(\"delta\").load(segment_effort_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#need to get distinct activity_ids and run them through the segment\n","#we make 1 api query in the first script, so gonna be allowed 99 with this run "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#get all of the activity_ids, limit to 15 as thats how many we can run in a single query\n","#also need to query the activities we have written to segment storage, so as not to repeat \n","#Strava API usage is limited on a per-application basis using both a 15-minute and daily request limit. The default rate limit allows 100 requests every 15 minutes, \n","# with up to 1,000 requests per day.\n","\n","#compare current activites vs what is written, \n","#going to need to write some try and excepts for expecting these values in return "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"Query_Segments","notebookOrigID":1439106573975145,"widgets":{}},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"89bbb57337a288069efe3ede2e44e349d48d03d33172adbe5738fcfdbda01bd0"}}},"nbformat":4,"nbformat_minor":0}
