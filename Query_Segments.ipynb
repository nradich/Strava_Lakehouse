{"cells":[{"cell_type":"markdown","metadata":{},"source":["Run the config file to authenticate script and query Key Vault\n","\n","Second script. Run after 'Query_Activites'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%run config"]},{"cell_type":"markdown","metadata":{},"source":["Dependencies "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pyspark.sql.functions import * \n","import pandas as pd"]},{"cell_type":"markdown","metadata":{},"source":["### API Query to get more specific details for each activity, need to pass each activity off individually "]},{"cell_type":"markdown","metadata":{},"source":["Get full activity dataset from what is written in storage, should be all activites"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","full_activity_dataset = spark.read.format(\"delta\").load(historical_activity_id_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["full_activity_dataset.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["full_activity_dataset.select('ingested_at').distinct().show()"]},{"cell_type":"markdown","metadata":{},"source":["Ggrab all of the disinct activity IDs "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"920f4af1-ce5f-48a7-8372-4725ed606ca0","showTitle":false,"title":""}},"outputs":[],"source":["full_activity_ids = full_activity_dataset.select('activity_ids').distinct().rdd.flatMap(lambda x: x).collect()"]},{"cell_type":"markdown","metadata":{},"source":["Take unique activity ids, and extract all of the segments associated with those activities"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def query_segments(activity_ids):\n","    \"\"\"Gets all segment_ids for each activity_id submitted\n","    Returns distinct values\"\"\"\n","    df = pd.DataFrame()\n","    activity_id_list =[]\n","    segment_id_list =[]\n","    for id in activity_ids:\n","        activity_id_urls = (\"{}{}?include_all_efforts= True\").format(\"https://www.strava.com/api/v3/activities/\",id)\n","        header = {'Authorization': 'Bearer ' + access_token}\n","        param = {'per_page': 200, 'page': 1}\n","        my_activity = requests.get(activity_id_urls, headers=header, params=param).json()\n","\n","        segment_effort_count =  len(my_activity['segment_efforts'])\n","        count = 0\n","        while count < segment_effort_count:\n","\n","            activity_id = my_activity['segment_efforts'][count]['activity']['id']\n","            segment_id = my_activity['segment_efforts'][count]['id']\n","            activity_id_list.append(activity_id)\n","            segment_id_list.append(segment_id)\n","                  \n","            columns = ['segment_id', 'activity_id']\n","            extracted_data = [segment_id_list, activity_id_list]\n","            segment_df = pd.DataFrame.from_dict(dict(zip(columns, extracted_data)))\n","\n","            df = pd.concat([df, segment_df])\n","\n","            #df= df.append(segment_df)\n","            count += 1\n","\n","    #convert pandas df to spark\n","        \n","    segment_spark_df = spark.createDataFrame(df)\n","\n","    #drop duplicate entries\n","    segment_spark_df = segment_spark_df.dropDuplicates()\n","\n","    segment_spark_df = segment_spark_df.select(concat(segment_spark_df.segment_id,segment_spark_df.activity_id).alias(\"Activity_Segment_JointID\"), 'segment_id','activity_id')\n","\n","    segment_spark_df = segment_spark_df.withColumn(\"ingest_file_name\", lit(\"segment_efforts_ids\")) \\\n","                                .withColumn(\"ingested_at\", lit(current_timestamp()))\n","\n","    return segment_spark_df"]},{"cell_type":"markdown","metadata":{},"source":["Need to compare activites already stored with segments as there are limits for strava API\n","\n","Initially will go 99 request, but might need to reduce that to save requests for segments"]},{"cell_type":"markdown","metadata":{},"source":["Query all segments from all activities "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#testing segment query on small list of activities\n","test_id_list = full_activity_ids[:5]\n","segment_id_df = query_segments(test_id_list)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["segment_id_df.show(5)"]},{"cell_type":"markdown","metadata":{},"source":["Not all activities register segments, will need to append into final DF that is writtent to storage with placerholder values so as not to keep querying them"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["segment_id_df.select(\"activity_id\").distinct().show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["try:\n","    #Query path, see if there are any activities with their associated segments written to storage\n","    segments_in_storage = spark.read.format(\"delta\").load(segment_effort_path)\n","except:\n","    #if that errors, meaning first time running the script\n","    #Write the first 99 activites to storage, will need to specificy sort order \n","    top_99_activity_ids = \n","    write_dataframe_to_storage(historical_df_to_write,storagepath, \"mergeSchema\", \"append\" )\n"]},{"cell_type":"markdown","metadata":{},"source":["Need to limit to 100 request as the api throws an errors after"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["write_dataframe_to_storage(segment_id_df,segment_effort_path, \"overwriteSchema\",\"overwrite\" )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["segments_in_storage = spark.read.format(\"delta\").load(segment_effort_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#need to get distinct activity_ids and run them through the segment\n","#we make 1 api query in the first script, so gonna be allowed 99 with this run "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#get all of the activity_ids, limit to 15 as thats how many we can run in a single query\n","#also need to query the activities we have written to segment storage, so as not to repeat \n","#Strava API usage is limited on a per-application basis using both a 15-minute and daily request limit. The default rate limit allows 100 requests every 15 minutes, \n","# with up to 1,000 requests per day.\n","\n","#compare current activites vs what is written, \n","#going to need to write some try and excepts for expecting these values in return "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"Query_Segments","notebookOrigID":1439106573975145,"widgets":{}},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"89bbb57337a288069efe3ede2e44e349d48d03d33172adbe5738fcfdbda01bd0"}}},"nbformat":4,"nbformat_minor":0}
