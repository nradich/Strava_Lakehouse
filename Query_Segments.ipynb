{"cells":[{"cell_type":"markdown","metadata":{},"source":["Run the config file to authenticate script and query Key Vault\n","\n","Second script. Run after 'Query_Activites'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%run config"]},{"cell_type":"markdown","metadata":{},"source":["Dependencies "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pyspark.sql.functions import * \n","from pyspark.sql import functions as F\n","from pyspark.sql import Row\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{},"source":["### API Query to get more specific details for each activity, need to pass each activity off individually "]},{"cell_type":"markdown","metadata":{},"source":["Get full activity dataset from what is written in storage, should be all activites"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","full_activity_dataset = spark.read.format(\"delta\").load(historical_activity_id_path)"]},{"cell_type":"markdown","metadata":{},"source":["Grab all of the disinct activity IDs "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"920f4af1-ce5f-48a7-8372-4725ed606ca0","showTitle":false,"title":""}},"outputs":[],"source":["full_activity_ids = full_activity_dataset.select('activity_ids').distinct().rdd.flatMap(lambda x: x).collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def query_segments(activity_ids : list):\n","    \"\"\"Gets all segment_ids for each activity_id submitted\n","    Returns distinct values\"\"\"\n","    df = pd.DataFrame()\n","    activity_id_list =[]\n","    segment_id_list =[]\n","    for id in activity_ids:\n","        activity_id_urls = (\"{}{}?include_all_efforts= True\").format(\"https://www.strava.com/api/v3/activities/\",id)\n","        header = {'Authorization': 'Bearer ' + access_token}\n","        param = {'per_page': 200, 'page': 1}\n","        my_activity = requests.get(activity_id_urls, headers=header, params=param).json()\n","\n","        segment_effort_count =  len(my_activity['segment_efforts'])\n","        count = 0\n","        while count < segment_effort_count:\n","\n","            activity_id = my_activity['segment_efforts'][count]['activity']['id']\n","            segment_id = my_activity['segment_efforts'][count]['id']\n","            activity_id_list.append(activity_id)\n","            segment_id_list.append(segment_id)\n","                  \n","            columns = ['segment_id', 'activity_id']\n","            extracted_data = [segment_id_list, activity_id_list]\n","            segment_df = pd.DataFrame.from_dict(dict(zip(columns, extracted_data)))\n","\n","            df = pd.concat([df, segment_df])\n","\n","            count += 1\n","\n","    #convert pandas df to spark\n","        \n","    segment_spark_df = spark.createDataFrame(df)\n","\n","    #drop duplicate entries\n","    segment_spark_df = segment_spark_df.dropDuplicates()\n","\n","    segment_spark_df = segment_spark_df.select(concat(segment_spark_df.segment_id,segment_spark_df.activity_id).alias(\"Activity_Segment_JointID\"), 'segment_id','activity_id')\n","\n","    segment_spark_df = segment_spark_df.withColumn(\"ingest_file_name\", lit(\"segment_efforts_ids\")) \\\n","                                .withColumn(\"ingested_at\", lit(current_timestamp()))\n","\n","    return segment_spark_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#will need to compare the activity ids that have already been queried for their segments\n","segments_in_storage = spark.read.format(\"delta\").load(segment_effort_path)\n","activity_ids_with_queried_segments = segments_in_storage.select('activity_id').distinct().rdd.flatMap(lambda x: x).collect()\n","\n","\n","activity_ids_without_queried_segments = [x for x in full_activity_ids  if x not in activity_ids_with_queried_segments ]\n","\n","#grab the first 99 spots so as not to overload the api call\n","eligible_activities = activity_ids_without_queried_segments[:99]\n","\n","\n","\n","segment_id_df = query_segments(eligible_activities)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#activities that returned segments\n","#need to add in the lambda to get the row values\n","returned_activity_ids = segment_id_df.select(\"activity_id\").distinct().rdd.flatMap(lambda x: x).collect()\n","\n","#activies submitted, that did not return segments\n","activity_ids_without_segments = [x for x in eligible_activities if x not in returned_activity_ids ]\n","\n","#Append in activies without segments to the DF\n","#convert the list of ids_without segments into a DF\n","#need to adjust column names \n","rows = [Row(Activity_Segment_JointID=i,  activity_id = i) for i in activity_ids_without_segments]\n","new_df = spark.createDataFrame(rows)\n","new_df = new_df.withColumn(\"ingest_file_name\", lit(\"segment_efforts_ids\")) \\\n","                                .withColumn(\"ingested_at\", lit(current_timestamp()))\\\n","                                .withColumn(\"segment_id\", lit(None).cast(\"long\"))\n","\n","#reorder columns to union into \n","new_df_reordered = new_df.select(*segment_id_df.columns)\n","\n","#union the two datasets together\n","all_segment_ids = segment_id_df.union(new_df_reordered)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["segment_id_df.show(20)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["value_counts = segment_id_df.groupBy(\"ingested_at\").agg(F.count(\"ingested_at\").alias(\"count\")).orderBy(\"count\", ascending=False)\n","value_counts.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_segment_ids.select(\"activity_id\").distinct().count()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["write_dataframe_to_storage(all_segment_ids,segment_effort_path, \"mergeSchema\", \"append\" )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_segment_ids.show(20)"]},{"cell_type":"markdown","metadata":{},"source":["Take unique activity ids, and extract all of the segments associated with those activities"]},{"cell_type":"markdown","metadata":{},"source":["Need to compare activites already stored with segments as there are limits for strava API\n","\n","Initially will go 99 request, but might need to reduce that to save requests for segments"]},{"cell_type":"markdown","metadata":{},"source":["Query all segments from all activities "]},{"cell_type":"markdown","metadata":{},"source":["Not all activities register segments, will need to append into final DF that is writtent to storage with placerholder values so as not to keep querying them"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["segment_id_df.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_segment_ids.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def write_dataframe_to_storage(dataset, storage_path, option, mode ):\n","    \"\"\"Function to write activity ids to storage. Will overwrite current delta file in storage\n","    Option refers to schema overwriteSchema or mergeSchema, mode being either overwrite or append\"\"\"\n","    dataset.write.format(\"delta\")\\\n","    .option(option, \"true\")\\\n","    .mode(mode)\\\n","    .save(storage_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["segments_in_storage = spark.read.format(\"delta\").load(segment_effort_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["write_dataframe_to_storage(all_segment_ids,segment_effort_path, \"mergeSchema\", \"append\" )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["segments_in_storage.show(10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["try:\n","    #Query path, see if there are any activities with their associated segments written to storage\n","    segments_in_storage = spark.read.format(\"delta\").load(segment_effort_path)\n","except:\n","    #if that errors, meaning first time running the script\n","    #Write the first 99 activites to storage, will need to specificy sort order \n","    top_99_activity_ids = \n","    write_dataframe_to_storage(historical_df_to_write,storagepath, \"mergeSchema\", \"append\" )\n"]},{"cell_type":"markdown","metadata":{},"source":["Need to limit to 100 request as the api throws an errors after"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["write_dataframe_to_storage(segment_id_df,segment_effort_path, \"overwriteSchema\",\"overwrite\" )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["segments_in_storage = spark.read.format(\"delta\").load(segment_effort_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#need to get distinct activity_ids and run them through the segment\n","#we make 1 api query in the first script, so gonna be allowed 99 with this run "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#get all of the activity_ids, limit to 15 as thats how many we can run in a single query\n","#also need to query the activities we have written to segment storage, so as not to repeat \n","#Strava API usage is limited on a per-application basis using both a 15-minute and daily request limit. The default rate limit allows 100 requests every 15 minutes, \n","# with up to 1,000 requests per day.\n","\n","#compare current activites vs what is written, \n","#going to need to write some try and excepts for expecting these values in return "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"Query_Segments","notebookOrigID":1439106573975145,"widgets":{}},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"89bbb57337a288069efe3ede2e44e349d48d03d33172adbe5738fcfdbda01bd0"}}},"nbformat":4,"nbformat_minor":0}
