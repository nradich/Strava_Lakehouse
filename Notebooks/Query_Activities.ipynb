{"cells":[{"cell_type":"markdown","metadata":{},"source":["Run the config file to authenticate the machine and query variables from Key Vault"]},{"cell_type":"code","execution_count":52,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"17d5c4fa-0e12-483c-a9dc-a0da2cd10381","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Code from file 'file:///c%3A/Users/nicholas.radich/Documents/Strava_Lakehouse/config.py':\n"," client_id = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"clientid\") \r\n","client_secret = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"clientsecret\") \r\n","new_refresh_token = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"newrefreshtoken\")\r\n","activity_id_path = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"activityidpath\") \r\n","historical_activity_id_path = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"historicalactivitydfpath\") \r\n","segment_effort_path = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"segmenteffortpath\") \r\n","segment_details_path = dbutils.secrets.get(scope = \"key_vault_secrets\", key = \"segmentdetailspath\") \r\n","\r\n","\r\n","\r\n","import requests\r\n","import urllib3\r\n","urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\r\n","\r\n","auth_url = \"https://www.strava.com/oauth/token\"\r\n","activites_url = \"https://www.strava.com/api/v3/athlete/activities\"\r\n","\r\n","\r\n","payload = {\r\n","    'client_id':  client_id,\r\n","    'client_secret': client_secret,\r\n","    'refresh_token': new_refresh_token,\r\n","    'grant_type': 'refresh_token',\r\n","    'f': 'json'\r\n","}\r\n","\r\n","res = requests.post(auth_url, data=payload, verify=False)\r\n","access_token = res.json()['access_token']"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":[]}],"source":["%run config"]},{"cell_type":"markdown","metadata":{},"source":["Depedencies "]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[]}],"source":["from pyspark.sql.functions import * \n","from pyspark.sql.types import LongType\n","import pandas as pd\n","import requests \n","import utils"]},{"cell_type":"markdown","metadata":{},"source":["API call to grab all of the acitivites within a personal account"]},{"cell_type":"markdown","metadata":{},"source":["Make API call to get all strava activites"]},{"cell_type":"code","execution_count":54,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6d9ca4f7-85af-4730-9cc0-d534ef0a9221","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":[]}],"source":["my_dataset = utils.activity_api_call(access_token)"]},{"cell_type":"markdown","metadata":{},"source":["Extracts activity ids and supporting information from initial dataset"]},{"cell_type":"markdown","metadata":{},"source":["Extract activity IDs and return refined columns "]},{"cell_type":"code","execution_count":55,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2459ffb8-1a9a-4a03-92fa-a7c86cac5fcf","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":[]}],"source":["\n","activity_id_DF, activity_df = utils.extract_activities(my_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["Function to write datasets to storage, delta format"]},{"cell_type":"markdown","metadata":{},"source":["Write Activity_ID_DF to storage"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[]}],"source":["#downside dataframes for testing purpose\n","activity_df= activity_df.limit(10)\n","activity_id_DF = activity_id_DF.limit(10)"]},{"cell_type":"code","execution_count":57,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ff90bbb6-be4b-462e-953f-bebff06fcbd8","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":[]}],"source":["#write the activity IDs to storage\n","utils.write_dataframe_to_storage(activity_id_DF,activity_id_path, \"overwriteSchema\",\"overwrite\" )"]},{"cell_type":"markdown","metadata":{},"source":["Read the activities from storage"]},{"cell_type":"code","execution_count":58,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"38b7d57a-e371-40b5-827d-cdd37122276b","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":[]}],"source":["#read the activities from storage\n","stored_activity_ids = spark.read.format(\"delta\").load(activity_id_path)"]},{"cell_type":"markdown","metadata":{},"source":["Compare what is currently in storage vs the most recent API call\n","If nothing is written to storage ie first run, still need to execute and write original DF to storage"]},{"cell_type":"markdown","metadata":{},"source":["Write full dataset to storage, will check to see if data already exists at path "]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[]}],"source":["historical_activites = utils.get_historical_dataset(historical_activity_id_path, activity_df, historical_activity_id_path)"]},{"cell_type":"markdown","metadata":{},"source":["Extract those activites not currently in storage"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[]}],"source":["#Extract activity ids from dataframes to list, to make comparison \n","#activity_id_list = activity_id_DF.select('activity_ids').distinct().collect()\n","#from full dataset, all unique activity ids\n","activity_id_list = stored_activity_ids.select('activity_id').distinct().rdd.flatMap(lambda x: x).collect()\n","\n","#currently what is written in storage, activity IDs\n","historical_activity_id_list = historical_activites.select('activity_ids').distinct().rdd.flatMap(lambda x: x).collect()\n","\n","#make comparison between two, and find those IDs not written to storage\n","activity_ids_not_written_to_storage = [x for x in activity_id_list if x not in historical_activity_id_list ]\n","\n","#filter original DF with those ids not currently in storage\n","new_activities = activity_df.filter(activity_df.activity_ids.isin(activity_ids_not_written_to_storage))"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[]}],"source":["#append new activity IDs to the storage location\n","utils.write_dataframe_to_storage(new_activities, historical_activity_id_path,\"mergeSchema\", \"append\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df1 = spark.read.format(\"delta\").load(historical_activity_id_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#only adds in distinct activities\n","from pyspark.sql import functions as F\n","df1.orderBy(F.desc('activity_ids')).show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["activity_id_DF.select('activity_id').show()"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"Query_Activities","notebookOrigID":4431027783720630,"widgets":{}},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"vscode":{"interpreter":{"hash":"89bbb57337a288069efe3ede2e44e349d48d03d33172adbe5738fcfdbda01bd0"}}},"nbformat":4,"nbformat_minor":0}
